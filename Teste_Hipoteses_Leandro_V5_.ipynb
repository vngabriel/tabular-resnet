{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeb69a6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a319f2f1-a465-4627-8470-4581f421c8b8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_lightgbm = pd.read_csv('/content/lightgbm_metrics.csv')\n",
        "df_catboost = pd.read_csv('/content/catboost_metrics.csv')\n",
        "df_xgboost = pd.read_csv('/content/xgboost_metrics.csv')\n",
        "df_resnet = pd.read_csv('/content/resnet_metrics.csv')\n",
        "df_autogluon = pd.read_csv('/content/autogluon_metrics.csv')\n",
        "df_autosklearn = pd.read_csv('/content/autosklearn_metrics.csv')\n",
        "\n",
        "print(\"Classifier metrics CSV files loaded into DataFrames: df_lightgbm, df_catboost, df_xgboost, df_resnet, df_autogluon, df_autosklearn.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier metrics CSV files loaded into DataFrames: df_lightgbm, df_catboost, df_xgboost, df_resnet, df_autogluon, df_autosklearn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd3d241a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4122b586-00f3-4bc7-d467-058553d61921"
      },
      "source": [
        "classifiers = ['lightgbm', 'catboost', 'xgboost', 'resnet', 'autogluon', 'autosklearn']\n",
        "\n",
        "classifier_dfs = {\n",
        "    'lightgbm': df_lightgbm,\n",
        "    'catboost': df_catboost,\n",
        "    'xgboost': df_xgboost,\n",
        "    'resnet': df_resnet,\n",
        "    'autogluon': df_autogluon,\n",
        "    'autosklearn': df_autosklearn\n",
        "}\n",
        "\n",
        "metric_mapping = {\n",
        "    'Mean AUC OVO': 'auc_ovo',\n",
        "    'Mean Accuracy (ACC)': 'accuracy',\n",
        "    'G-Mean': 'gmean',\n",
        "    'Mean Cross-Entropy (CE)': 'cross_entropy'\n",
        "}\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for output_metric_name, df_column_name in metric_mapping.items():\n",
        "    all_results[output_metric_name] = {}\n",
        "    for classifier_name in classifiers:\n",
        "        df = classifier_dfs[classifier_name]\n",
        "        # Extract the column of values and convert to a list\n",
        "        # Assuming each DataFrame contains 30 rows corresponding to the 30 performance values\n",
        "        all_results[output_metric_name][classifier_name] = df[df_column_name].tolist()\n",
        "\n",
        "print(\"Data restructured into 'all_results' dictionary.\")\n",
        "# Display a sample of the structure for verification\n",
        "print(f\"Example for 'Mean AUC OVO' and 'lightgbm': {all_results['Mean AUC OVO']['lightgbm'][:5]}...\")\n",
        "print(f\"Number of values for 'Mean AUC OVO' and 'lightgbm': {len(all_results['Mean AUC OVO']['lightgbm'])}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data restructured into 'all_results' dictionary.\n",
            "Example for 'Mean AUC OVO' and 'lightgbm': [0.9997125446962574, 0.9989591444547612, 0.5750249889163095, 0.9613442632688824, 1.0]...\n",
            "Number of values for 'Mean AUC OVO' and 'lightgbm': 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a98988a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378f5111-7f9d-4fc2-e8b7-6d4317434d7c"
      },
      "source": [
        "!pip install scikit-posthocs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import scikit_posthocs as sp\n",
        "from scipy.stats import f as f_dist\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIDENCE INTERVALS (95%)\n",
        "# =============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"CALCULATING 95% CONFIDENCE INTERVALS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "confidence_results = []\n",
        "\n",
        "for metric_name, metric_results in all_results.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"METRIC: {metric_name.upper()}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    for classifier_name in classifiers:\n",
        "        values = metric_results[classifier_name]\n",
        "        n_samples = len(values)\n",
        "\n",
        "        if n_samples > 1:\n",
        "            arr = np.array(values)\n",
        "            mean_val = np.mean(arr)\n",
        "            std_val = np.std(arr, ddof=1) # Use ddof=1 for sample standard deviation\n",
        "\n",
        "            # Confidence Interval\n",
        "            confidence_level = 0.95\n",
        "            degrees_freedom = n_samples - 1\n",
        "            confidence_interval = stats.t.interval(\n",
        "                confidence_level,\n",
        "                degrees_freedom,\n",
        "                loc=mean_val,\n",
        "                scale=stats.sem(arr)\n",
        "            )\n",
        "\n",
        "            confidence_results.append({\n",
        "                'metric': metric_name,\n",
        "                'classifier': classifier_name,\n",
        "                'mean': mean_val,\n",
        "                'std': std_val,\n",
        "                'ci_lower': confidence_interval[0],\n",
        "                'ci_upper': confidence_interval[1]\n",
        "            })\n",
        "\n",
        "            print(f\"{classifier_name:15} (N={n_samples}) Mean: {mean_val:.4f} | Std: {std_val:.4f} | \"\n",
        "                  f\"CI 95%: [{confidence_interval[0]:.4f}, {confidence_interval[1]:.4f}]\")\n",
        "\n",
        "df_confidence = pd.DataFrame(confidence_results)\n",
        "\n",
        "# =============================================================================\n",
        "# FRIEDMAN TEST AND NEMENYI POST-HOC TEST\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PERFORMING FRIEDMAN TEST AND NEMENYI POST-HOC TEST\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for metric_name, metric_results in all_results.items():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"ANALYSIS FOR METRIC: {metric_name.upper()}\")\n",
        "    print(\"=\"*80)\n",
        "    print()\n",
        "\n",
        "    # Data Matrix: Rows = 30 Repetitions, Columns = Classifiers\n",
        "    # Ensure classifiers are in a consistent order\n",
        "    data_matrix = np.array([metric_results[clf] for clf in classifiers]).T\n",
        "    n_datasets, n_classifiers = data_matrix.shape\n",
        "\n",
        "    print(f\"Configuration: {n_datasets} repetitions \\u00d7 {n_classifiers} classifiers\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # STEP 1: Calculate ranks\n",
        "    # -------------------------------------------------------------------------\n",
        "    ranks_matrix = np.zeros_like(data_matrix, dtype=float)\n",
        "    for i in range(n_datasets):\n",
        "        if metric_name == \"Mean Cross-Entropy (CE)\":\n",
        "            # Lower Cross-Entropy is better, so rank directly (ascending)\n",
        "            ranks_matrix[i, :] = stats.rankdata(data_matrix[i, :], method='average')\n",
        "        else:\n",
        "            # Higher values (AUC, Accuracy, G-Mean) are better, so rank descending\n",
        "            ranks_matrix[i, :] = stats.rankdata(-data_matrix[i, :], method='average')\n",
        "\n",
        "    mean_ranks = np.mean(ranks_matrix, axis=0)\n",
        "\n",
        "    rank_df = pd.DataFrame({\n",
        "        'Classificador': classifiers,\n",
        "        'Rank M\\u00e9dio': mean_ranks\n",
        "    }).sort_values('Rank M\\u00e9dio')\n",
        "\n",
        "    print(\"\\n\\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\")\n",
        "    print(\"\\u2502  MEAN RANKS (Lower Rank = Better Performance)          \\u2502\")\n",
        "    print(\"\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\")\n",
        "    print(rank_df.to_string(index=False))\n",
        "    print()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # STEP 2: Friedman + Iman-Davenport Test\n",
        "    # -------------------------------------------------------------------------\n",
        "    k = n_classifiers\n",
        "    N = n_datasets\n",
        "\n",
        "    # Original Friedman statistic\n",
        "    # R_j_mean_sq_sum = np.sum(mean_ranks**2)\n",
        "    # chi2_F_original = (12 * N / (k * (k + 1))) * (R_j_mean_sq_sum - (k * (k + 1)**2) / 4)\n",
        "\n",
        "    # Iman-Davenport F-statistic\n",
        "    # Calculate Friedman Chi-square directly from mean ranks\n",
        "    R_squared_sum = np.sum(mean_ranks**2)\n",
        "    chi_sq_friedman = (12 * N) / (k * (k + 1)) * (R_squared_sum - (k * (k + 1)**2) / 4)\n",
        "\n",
        "    if (N * (k - 1) - chi_sq_friedman) == 0:\n",
        "        # This can happen if chi_sq_friedman is exactly N * (k-1), which is theoretical maximum\n",
        "        F_F = np.inf if chi_sq_friedman > 0 else 0 # Or handle as an edge case where no variance\n",
        "    else:\n",
        "        F_F = ((N - 1) * chi_sq_friedman) / (N * (k - 1) - chi_sq_friedman)\n",
        "\n",
        "    df1 = k - 1\n",
        "    df2 = (k - 1) * (N - 1)\n",
        "    p_value_friedman = 1 - f_dist.cdf(F_F, df1, df2)\n",
        "\n",
        "    print(\"\\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\")\n",
        "    print(\"\\u2502  FRIEDMAN TEST (Iman-Davenport)                             \\u2502\")\n",
        "    print(\"\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\")\n",
        "    print(f\"  F_F: {F_F:.4f} | p-value: {p_value_friedman:.4f}\")\n",
        "\n",
        "    if p_value_friedman < 0.05:\n",
        "        print(f\"  \\u2713 Reject H0: There is a statistically significant difference between models.\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # STEP 3: Nemenyi Post-Hoc Test\n",
        "        # ---------------------------------------------------------------------\n",
        "        print(\"\\n\\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\")\n",
        "        print(\"\\u2502  NEMENYI POST-HOC TEST                                      \\u2502\")\n",
        "        print(\"\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\")\n",
        "\n",
        "        # scikit_posthocs.posthoc_nemenyi_friedman expects the data in a specific format:\n",
        "        # each row is a sample (dataset/repetition), each column is a group (classifier)\n",
        "        # So, the data_matrix is already in the correct format (N rows x k columns)\n",
        "        # It also implicitly handles ranking internally, consistent with Friedman.\n",
        "        pairwise_pvalues = sp.posthoc_nemenyi_friedman(data_matrix)\n",
        "        pairwise_pvalues.index = classifiers\n",
        "        pairwise_pvalues.columns = classifiers\n",
        "\n",
        "        print(f\"Matrix of Nemenyi p-Values:\")\n",
        "        print(pairwise_pvalues)\n",
        "        print(\"\\nPairs with significant difference (p < 0.05):\")\n",
        "        found_significant = False\n",
        "        for i in range(len(classifiers)): # Loop through rows (classifiers)\n",
        "            for j in range(i + 1, len(classifiers)): # Loop through columns (other classifiers)\n",
        "                p_val = pairwise_pvalues.iloc[i, j]\n",
        "                if p_val < 0.05:\n",
        "                    found_significant = True\n",
        "                    # Determine which classifier is better based on mean ranks\n",
        "                    # Ensure mean_ranks corresponds to the order of 'classifiers'\n",
        "                    rank_clf_i = mean_ranks[classifiers.index(pairwise_pvalues.index[i])]\n",
        "                    rank_clf_j = mean_ranks[classifiers.index(pairwise_pvalues.columns[j])]\n",
        "\n",
        "                    if rank_clf_i < rank_clf_j:\n",
        "                        better_clf = pairwise_pvalues.index[i]\n",
        "                        worse_clf = pairwise_pvalues.columns[j]\n",
        "                    else:\n",
        "                        better_clf = pairwise_pvalues.columns[j]\n",
        "                        worse_clf = pairwise_pvalues.index[i]\n",
        "\n",
        "                    print(f\"  \\u2713 {better_clf} vs {worse_clf} (p={p_val:.4f}) -> {better_clf} performs significantly better\")\n",
        "\n",
        "        if not found_significant:\n",
        "            print(\"  No significant pairwise differences found by Nemenyi.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"  \\u2717 Do not reject H0. The models perform equivalently.\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "\n",
        "print(\"Statistical analysis complete.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-posthocs in /usr/local/lib/python3.12/dist-packages (0.11.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from scikit-posthocs) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from scikit-posthocs) (1.16.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (from scikit-posthocs) (0.14.5)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from scikit-posthocs) (2.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from scikit-posthocs) (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from scikit-posthocs) (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.20.0->scikit-posthocs) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.20.0->scikit-posthocs) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.20.0->scikit-posthocs) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scikit-posthocs) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scikit-posthocs) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scikit-posthocs) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scikit-posthocs) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scikit-posthocs) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scikit-posthocs) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scikit-posthocs) (3.2.5)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels->scikit-posthocs) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.20.0->scikit-posthocs) (1.17.0)\n",
            "================================================================================\n",
            "CALCULATING 95% CONFIDENCE INTERVALS\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "METRIC: MEAN AUC OVO\n",
            "============================================================\n",
            "\n",
            "lightgbm        (N=30) Mean: 0.8876 | Std: 0.1203 | CI 95%: [0.8427, 0.9325]\n",
            "catboost        (N=30) Mean: 0.8855 | Std: 0.1283 | CI 95%: [0.8376, 0.9334]\n",
            "xgboost         (N=30) Mean: 0.8838 | Std: 0.1277 | CI 95%: [0.8362, 0.9315]\n",
            "resnet          (N=30) Mean: 0.8911 | Std: 0.1261 | CI 95%: [0.8440, 0.9382]\n",
            "autogluon       (N=30) Mean: 0.8906 | Std: 0.1272 | CI 95%: [0.8430, 0.9381]\n",
            "autosklearn     (N=30) Mean: 0.8722 | Std: 0.1367 | CI 95%: [0.8211, 0.9233]\n",
            "\n",
            "============================================================\n",
            "METRIC: MEAN ACCURACY (ACC)\n",
            "============================================================\n",
            "\n",
            "lightgbm        (N=30) Mean: 0.8307 | Std: 0.1689 | CI 95%: [0.7677, 0.8938]\n",
            "catboost        (N=30) Mean: 0.8320 | Std: 0.1704 | CI 95%: [0.7684, 0.8956]\n",
            "xgboost         (N=30) Mean: 0.8279 | Std: 0.1696 | CI 95%: [0.7646, 0.8912]\n",
            "resnet          (N=30) Mean: 0.8270 | Std: 0.1825 | CI 95%: [0.7589, 0.8952]\n",
            "autogluon       (N=30) Mean: 0.8433 | Std: 0.1674 | CI 95%: [0.7808, 0.9059]\n",
            "autosklearn     (N=30) Mean: 0.8485 | Std: 0.1666 | CI 95%: [0.7863, 0.9107]\n",
            "\n",
            "============================================================\n",
            "METRIC: G-MEAN\n",
            "============================================================\n",
            "\n",
            "lightgbm        (N=30) Mean: 0.7244 | Std: 0.2285 | CI 95%: [0.6391, 0.8097]\n",
            "catboost        (N=30) Mean: 0.7244 | Std: 0.2232 | CI 95%: [0.6410, 0.8077]\n",
            "xgboost         (N=30) Mean: 0.7343 | Std: 0.2070 | CI 95%: [0.6570, 0.8116]\n",
            "resnet          (N=30) Mean: 0.7850 | Std: 0.2015 | CI 95%: [0.7097, 0.8603]\n",
            "autogluon       (N=30) Mean: 0.7413 | Std: 0.2380 | CI 95%: [0.6525, 0.8302]\n",
            "autosklearn     (N=30) Mean: 0.7569 | Std: 0.2143 | CI 95%: [0.6769, 0.8369]\n",
            "\n",
            "============================================================\n",
            "METRIC: MEAN CROSS-ENTROPY (CE)\n",
            "============================================================\n",
            "\n",
            "lightgbm        (N=30) Mean: 0.3922 | Std: 0.3314 | CI 95%: [0.2684, 0.5160]\n",
            "catboost        (N=30) Mean: 0.3782 | Std: 0.3269 | CI 95%: [0.2562, 0.5003]\n",
            "xgboost         (N=30) Mean: 0.4082 | Std: 0.3401 | CI 95%: [0.2812, 0.5352]\n",
            "resnet          (N=30) Mean: 0.4012 | Std: 0.3726 | CI 95%: [0.2621, 0.5403]\n",
            "autogluon       (N=30) Mean: 0.4013 | Std: 0.3290 | CI 95%: [0.2785, 0.5242]\n",
            "autosklearn     (N=30) Mean: 0.8872 | Std: 1.7659 | CI 95%: [0.2278, 1.5466]\n",
            "\n",
            "================================================================================\n",
            "PERFORMING FRIEDMAN TEST AND NEMENYI POST-HOC TEST\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS FOR METRIC: MEAN AUC OVO\n",
            "================================================================================\n",
            "\n",
            "Configuration: 30 repetitions × 6 classifiers\n",
            "\n",
            "┌────────────────────────────────────────────────────────┐\n",
            "│  MEAN RANKS (Lower Rank = Better Performance)          │\n",
            "└────────────────────────────────────────────────────────┘\n",
            "Classificador  Rank Médio\n",
            "       resnet    3.033333\n",
            "  autosklearn    3.266667\n",
            "    autogluon    3.433333\n",
            "     catboost    3.433333\n",
            "     lightgbm    3.650000\n",
            "      xgboost    4.183333\n",
            "\n",
            "┌─────────────────────────────────────────────────────────────┐\n",
            "│  FRIEDMAN TEST (Iman-Davenport)                             │\n",
            "└─────────────────────────────────────────────────────────────┘\n",
            "  F_F: 1.3357 | p-value: 0.2524\n",
            "  ✗ Do not reject H0. The models perform equivalently.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS FOR METRIC: MEAN ACCURACY (ACC)\n",
            "================================================================================\n",
            "\n",
            "Configuration: 30 repetitions × 6 classifiers\n",
            "\n",
            "┌────────────────────────────────────────────────────────┐\n",
            "│  MEAN RANKS (Lower Rank = Better Performance)          │\n",
            "└────────────────────────────────────────────────────────┘\n",
            "Classificador  Rank Médio\n",
            "  autosklearn    2.483333\n",
            "    autogluon    3.366667\n",
            "       resnet    3.650000\n",
            "     catboost    3.733333\n",
            "     lightgbm    3.833333\n",
            "      xgboost    3.933333\n",
            "\n",
            "┌─────────────────────────────────────────────────────────────┐\n",
            "│  FRIEDMAN TEST (Iman-Davenport)                             │\n",
            "└─────────────────────────────────────────────────────────────┘\n",
            "  F_F: 2.5751 | p-value: 0.0290\n",
            "  ✓ Reject H0: There is a statistically significant difference between models.\n",
            "\n",
            "┌─────────────────────────────────────────────────────────────┐\n",
            "│  NEMENYI POST-HOC TEST                                      │\n",
            "└─────────────────────────────────────────────────────────────┘\n",
            "Matrix of Nemenyi p-Values:\n",
            "             lightgbm  catboost   xgboost    resnet  autogluon  autosklearn\n",
            "lightgbm     1.000000  0.999948  0.999948  0.998982   0.928651     0.058266\n",
            "catboost     0.999948  1.000000  0.998450  0.999979   0.974215     0.100193\n",
            "xgboost      0.999948  0.998450  1.000000  0.991951   0.849796     0.032104\n",
            "resnet       0.998982  0.999979  0.991951  1.000000   0.991951     0.150842\n",
            "autogluon    0.928651  0.974215  0.849796  0.991951   1.000000     0.447113\n",
            "autosklearn  0.058266  0.100193  0.032104  0.150842   0.447113     1.000000\n",
            "\n",
            "Pairs with significant difference (p < 0.05):\n",
            "  ✓ autosklearn vs xgboost (p=0.0321) -> autosklearn performs significantly better\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS FOR METRIC: G-MEAN\n",
            "================================================================================\n",
            "\n",
            "Configuration: 30 repetitions × 6 classifiers\n",
            "\n",
            "┌────────────────────────────────────────────────────────┐\n",
            "│  MEAN RANKS (Lower Rank = Better Performance)          │\n",
            "└────────────────────────────────────────────────────────┘\n",
            "Classificador  Rank Médio\n",
            "       resnet    2.616667\n",
            "  autosklearn    2.966667\n",
            "    autogluon    3.666667\n",
            "     lightgbm    3.800000\n",
            "     catboost    3.950000\n",
            "      xgboost    4.000000\n",
            "\n",
            "┌─────────────────────────────────────────────────────────────┐\n",
            "│  FRIEDMAN TEST (Iman-Davenport)                             │\n",
            "└─────────────────────────────────────────────────────────────┘\n",
            "  F_F: 2.9887 | p-value: 0.0134\n",
            "  ✓ Reject H0: There is a statistically significant difference between models.\n",
            "\n",
            "┌─────────────────────────────────────────────────────────────┐\n",
            "│  NEMENYI POST-HOC TEST                                      │\n",
            "└─────────────────────────────────────────────────────────────┘\n",
            "Matrix of Nemenyi p-Values:\n",
            "             lightgbm  catboost   xgboost    resnet  autogluon  autosklearn\n",
            "lightgbm     1.000000  0.999617  0.998450  0.139430   0.999785     0.514961\n",
            "catboost     0.999617  1.000000  0.999998  0.064018   0.991951     0.321939\n",
            "xgboost      0.998450  0.999998  1.000000  0.048050   0.983109     0.267036\n",
            "resnet       0.139430  0.064018  0.048050  1.000000   0.250066     0.978999\n",
            "autogluon    0.999785  0.991951  0.983109  0.250066   1.000000     0.696733\n",
            "autosklearn  0.514961  0.321939  0.267036  0.978999   0.696733     1.000000\n",
            "\n",
            "Pairs with significant difference (p < 0.05):\n",
            "  ✓ resnet vs xgboost (p=0.0481) -> resnet performs significantly better\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS FOR METRIC: MEAN CROSS-ENTROPY (CE)\n",
            "================================================================================\n",
            "\n",
            "Configuration: 30 repetitions × 6 classifiers\n",
            "\n",
            "┌────────────────────────────────────────────────────────┐\n",
            "│  MEAN RANKS (Lower Rank = Better Performance)          │\n",
            "└────────────────────────────────────────────────────────┘\n",
            "Classificador  Rank Médio\n",
            "     catboost    2.966667\n",
            "     lightgbm    3.133333\n",
            "  autosklearn    3.633333\n",
            "    autogluon    3.633333\n",
            "       resnet    3.700000\n",
            "      xgboost    3.933333\n",
            "\n",
            "┌─────────────────────────────────────────────────────────────┐\n",
            "│  FRIEDMAN TEST (Iman-Davenport)                             │\n",
            "└─────────────────────────────────────────────────────────────┘\n",
            "  F_F: 1.1764 | p-value: 0.3236\n",
            "  ✗ Do not reject H0. The models perform equivalently.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Statistical analysis complete.\n"
          ]
        }
      ]
    }
  ]
}